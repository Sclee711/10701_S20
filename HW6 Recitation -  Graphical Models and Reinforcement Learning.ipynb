{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Graphical Models\n",
    "The central question that graphical models try to answer is how to more compactly write the joint distribution $P(X_1, X_2, ..., X_n)$ by accounting for the conditional independence relationships between variables. This is referred to as \"factoring\" the joint distribution. To emphasize the utility of doing so, consider that representing the full joint distribution (necessary when each variables always depends on all other variables) requires representing exponentially many states. In practice, variables may have some independence structure or relatively limited interaction with one another. Taking advantage of these conditions enables us to represent the joint distribution more or less equivalently with significantly fewer parameters. We will review a few common examples of how this is done, and then go over a problem involving a specific Bayesian Network. While there are many types of graphical models, we will only focus on directed graphical models whose structure is a directed acyclic graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes\n",
    "Suppose that we are considering how to classify vectors $X \\in \\{0, 1\\}^d$ into binary labels $Y \\in \\{0, 1\\}$. In general, we are trying to model the distribution $P(Y|X_1,X_2,...,X_d)$ which can be related to the joint distribution via $P(X_1,X_2,...,X_d,Y) = P(Y|X_1,X_2,...,X_d)P(X_1,X_2,...,X_d)$. However, Naive Bayes relies on a particular conditional independence assumption, namely that the features are conditionally independent given the label. This assumption allows us to write the conditional distribution of $X|Y$ as $P(X_1,X_2,...,X_d|Y) = \\prod_{i=1}^d P(X_i|Y)$. The graphical model corresponding to Naive Bayes is as follows.\n",
    "\n",
    "<img src=\"figures/naive_bayes.png\">\n",
    "\n",
    "While the Naive Bayes assumption is unlikely to exactly hold, it gives us a model that is tractable to represent (requiring only $2n + 1$ variables in the given binary case) and fairly robust in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hidden Markov Models (HMMs)\n",
    "Here, we'll discuss another type of graphical model commonly used for modeling sequences of data. Suppose instead that we have a sequence $X \\in \\{0, 1\\}^n$ (unobserved \"hidden\" variables that we are trying to model) which produces an observable binary output $Y_i$ at each step $i$ conditioned on all previous information. We thus have a sequence of prediction problems based on the following joint distribution, which we will later show how to simplify.\n",
    "\n",
    "$$P(X_1,X_2,...,X_n,Y_1,Y_2,...,Y_n) = \\prod_{i=1}^n P(X_i, Y_i|X_{i-1}, Y_{i-1}, ...,X_1, Y_1)$$\n",
    "\n",
    "As we can see, the states of this model depend on all past data, which becomes larger with every step. A common simplification (such as that adopted by n-gram models for natural language) is to reduce that dependence to a fixed window of size K. We will show how HMMs significantly reduce the problem space by only using a window of size 1 and adding a Markov assumption on the observed outputs.\n",
    "\n",
    "First, we can apply the Markov assumption - that $X_i$ is dependent only on $X_{i=1}$ - to simplify the probability distribution of $P(X_i|X_{i-1}, Y_{i-1}...,X_1, Y_1) = P(X_i|X_{i-1})$. Next, we extend this assumption to $Y$ by making $Y_i$ conditionally independent of all previous information given $X_i$, and hence $P(Y_i|X_i,X_{i-1}, Y_{i-1}, ...,X_1, Y_1) = P(Y_i|X_i)$. The graphical model now takes on the following form.\n",
    "\n",
    "<img src=\"figures/hmm.png\">\n",
    "\n",
    "The joint distribution thus simplifies to $P(X_1,X_2,...,X_n,Y_1,Y_2,...,Y_n) = \\prod_{i=1}^n P(Y_i|X_i)P(X_i|X_{i-1})$. Inference can then be performed in linear time using a dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks and Conditional Independence Rules\n",
    "Consider the some conditional independence tests involving the two following Bayes Nets.\n",
    "<img src=\"figures/ci_example1.png\">\n",
    "#### Are A and B independent? (A $\\perp$ B?) \n",
    "Yes.\n",
    "\n",
    "#### Are A and B conditionally independent given C? (A $\\perp$ B $|$ C?)\n",
    "No. Because both A and B influence C, knowing B and C affects the distribution of A.\n",
    "\n",
    "<img src=\"figures/ci_example2.png\">\n",
    "\n",
    "#### Are E and F independent? (E $\\perp$ F?) \n",
    "No. Knowing F affects the possible values of D, which affects $P(E|D)$.\n",
    "\n",
    "#### Are E and F conditionally independent given D? (E $\\perp$ F $|$ D?)\n",
    "Yes. The values of E and F only depend on $P(E|D)$ and $P(F|D)$.\n",
    "\n",
    "\n",
    "There are two main rules that this example illustrates. The first involves the relationship between nodes and parents. The value of a node can be affected by conditioning on any descendent of its parents, because observing a parent's descendent can provide information about the parent. However, if the parent's value is already known, then the parent's non-shared descendents will have no effect on the given node. Finally, conditioning on all of a node's parents will make it conditionally independent of all of its non-descendents.  \n",
    "\n",
    "The second rule involves the relationship between nodes and their children. If the only observed node is one of the node's descendents then it becomes dependent on all ancestors of the observed node. This is because all ancestors of a node affect the value of the observed node. Suppose we restrict the discussion to only the node's immediate children. Observing any of its children will make the node dependent on their children's other ancestors. However, the effect can be limited the node's children are also observed. \n",
    "\n",
    "Putting these rules together helps us derive the concept of a Markov Blanket. Given the values of a node's parents, the values of its children and its children's parents, the node will be conditionally independent of all other nodes in the graph, which also forms the basis of Gibbs Sampling.\n",
    "\n",
    "<img src=\"figures/bayes_net.png\">\n",
    "\n",
    "#### What is the Markov Blanket of Alarm?\n",
    "This includes all nodes except for Report, which is not needed because Alarm is conditionally independent of Report given Earthquake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: K-ZigZag Network\n",
    "\n",
    "A $k$-$zigzag$ network has $k$ Boolean root variables and $k + 1$ Boolean leaf variables, where root $i$ is connected to leaves $i$ and $i + 1$. Here is an example for $k = 3$, where each $D_i$ represents a Boolean disease variable and each $S_j$ is a Boolean symptom variable:\n",
    "\n",
    "<img src=\"figures/k_zigzag.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does having symptom $S_4$ affect the probability of disease $D_1$? Why or why not?\n",
    "No, because $S_4$ is not a descendant of $D_1$, and $D_1$ is independent of its nondescendants given its parents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using only conditional probabilities from the model, express the probability of having symptom $S_1$ but not symptom $S_2$, given disease $D_1$.\n",
    "$P(S_1, \\neg S_2|D_1) = P(S_1|D_1)P(\\neg S_2|D_1) = P(S_1|D_1)\\sum_{d2} P(d_2)P(S_2| D_1, d_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $True/False$: Exact inference in a k-zigzag net can be done in time O(k).\n",
    "True, the K-zigzag net is a polytree. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose the values of all the symptom variables have been observed, and you would like to do Gibbs sampling on the disease variables (i.e., sample each variable given its Markov blanket). What is the largest number of nonevidence variables that have to be considered when sampling any particular disease variable? Explain your answer\n",
    "\n",
    "2 non-evidence variables - the Markov blanket consists of its children (2 evidence variables) and their other parents (2 non-evidence variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization (EM)\n",
    "So far, we've mainly been concerned with the question of how to perform inference when the graph and the underlying probabilities are already known. We now turn our attention to the task of learning the probability tables for a known graph structure from data. In particular, we consider the case where the data that we're learning from contains missing entries. It is fairly straightforward to learn these tables when the data is not missing entries; one simply populates the tables with the empirical averages via the principle of maximum of likelihood. How do we properly compute these averages with missing data? At a high level, the solution adopted by EM is to assign preliminary (partial) values to the missing entries, evaluate the resulting expected likelihood, and then iteratively re-assign these partial values to optimize the likelihood. Let's define the problem formally.\n",
    "\n",
    "The components of EM include a set of observed variables $X = \\{X_1, \\ldots, X_n\\}$, a set of unobserved (latent) variables $Z = \\{Z_1, \\ldots, Z_n\\}$ that we would like to infer, as well as a set of parameters $\\Theta$ that we're trying to estimate. In the context of Bayes Nets, the $Z$ values are the partial assignments of the unlabeled data, and $\\Theta$ the parameters of the probability tables that we would like to learn. Consider also a clustering example (which we will go over in this recitation) where $Z$ are the assignments of the data to clusters and the $\\Theta$ parameters are the centroids of each cluster. The EM algorithm iterates on the following two steps.\n",
    "   1. First, during the expectation phase, we calculate the partial assignments $Z$ based on the known parameters and use them to compute the expected log-likehood, $\\mathbb{E}_{P(Z|X, \\Theta)}[\\log P(X, Z | \\Theta)]$.\n",
    "   2. Then, during the maximization phase, we compute new values of the parameters $\\Theta$ that maximize the inner expected log-likelihood term, setting the parameters to $\\Theta = argmax_{\\Theta'} \\, \\mathbb{E}_{P(Z|X, \\Theta)}[\\log P(X, Z | \\Theta')]$.\n",
    "Let's explore these steps in greater detail with a clustering example.\n",
    "\n",
    "Consider the K-Means problem, which is the problem of assigning each data point to one of $K$ different clusters such that the overall squared distance between data points and their assigned cluster mean is minimized.\n",
    "$$ \\min_{Z, \\Theta} \\sum_{i=1}^n \\sum_{j=1}^K \\mathbf{1}[z_i == K] \\|x_i - \\theta_i\\|^2_2$$\n",
    "There is a simple greedy algorithm for K-Means (known as Lloyd's algorithm) which assigns each point to its closest cluster, recomputes the cluster centroids, and then repeats these steps until convergence. We will show that by modeling clusters as Gaussians of unit covariance, we can use the framework of EM to derive a \"soft\" version of the classic K-means algorithm that uses partial assignments instead of hard assignments.\n",
    "\n",
    "Suppose that we have observed data points $X \\in \\mathbb{R}^{n \\times d}$ that have an associated set of \"latent\" assignment variables $Z \\in \\{1, \\ldots, K\\}^n$ and cluster centroid parameters $\\Theta \\in \\mathbb{R}^{k \\times d}$. \n",
    "Assume that the likelihood of $x_i$ belong to a given cluster $k$ is a multivariate Gaussian with mean $\\theta_k$ and unit covariance.\n",
    "$$P(x_i, z_i = k | \\Theta) = \\frac{1}{(2\\pi)^\\frac{d}{2}\\|I\\|^2_2} \\exp\\left(-\\frac{1}{2}(x_i - \\theta_i)^T I^{-1} (x_i - \\theta_i)\\right) = \\frac{1}{(2\\pi)^\\frac{d}{2}} \\exp\\left(-\\frac{1}{2}\\|x_i - \\theta_i\\|^2_2\\right)$$\n",
    "We will now walk through the two EM steps in detail. For the expectation step, note that because the $x_i$'s and $z_i$'s corresponding to data point $i$ are independent of those of the other data points, we can rewrite $P(Z_1,\\ldots,Z_n|X_1,\\ldots,X_n, \\Theta) = \\prod_{i=1}^n P(Z_i|X_i, \\Theta)$ and $\\log(P(X_1, Z_1,\\ldots,X_n,Z_n | \\Theta)) = \\sum_{i=1}^n \\log(P(X_i, Z_i | \\Theta))$. This lets us rewrite the expectation term in a more convenient form.\n",
    "\n",
    "$$ \\begin{align} \\mathbb{E}_{P(Z|X, \\Theta)}[\\log P(X, Z | \\Theta)] &= \\sum_{z} P(Z_1,\\ldots,Z_n|X_1,\\ldots,X_n, \\Theta) \\log(P(X_1, Z_1,\\ldots,X_n,Z_n | \\Theta)) \\\\  &= \\sum_{z} \\prod_{i=1}^n P(Z_i|X_i, \\Theta) \\sum_{i=1}^n \\log(P(X_i, Z_i | \\Theta)) \\\\ &= \\sum_{i=1}^n \\sum_{k=1}^K P(z_i = k | x_i, \\Theta) \\log P(x_i, z_i = k | \\Theta) \\end{align}$$\n",
    "\n",
    "\n",
    "First we will compute the partial assignment of each $Z_i$ using the definition of conditional probability.\n",
    "\n",
    "$$\\begin{align} P(z_i = k|x_i, \\Theta) &= \\frac{P(x_i, z_i = k | \\Theta)}{\\sum_{j=1}^K P(x_i, z_i = j | \\Theta)} \\\\ &= \\frac{\\frac{1}{(2\\pi)^\\frac{d}{2}} \\exp\\left(-\\frac{1}{2}\\|x_i - \\theta_k\\|^2_2\\right)}{\\sum_{j=1}^K \\frac{1}{(2\\pi)^\\frac{d}{2}} \\exp\\left(-\\frac{1}{2}\\|x_i - \\theta_j\\|^2_2\\right)}  \\\\ &= \\frac{\\exp\\left(-\\frac{1}{2}\\|x_i - \\theta_k\\|^2_2\\right)}{\\sum_{j=1}^K \\exp\\left(-\\frac{1}{2}\\|x_i - \\theta_j\\|^2_2\\right)} \\\\ &= \\delta(z_i = k, x_i, \\Theta) \\end{align}$$\n",
    "\n",
    "Note the resemblance to the softmax function. Because these terms remain fixed during the maximization step, we will denote them as $\\delta(z_i = k, x_i, \\Theta)$. We then use the partial assignments of $z_i$ to compute the expected log-likelihood.\n",
    "\n",
    "$$\\begin{align} \\mathbb{E}_{P(Z|X, \\Theta)}[\\log P(X, Z | \\Theta)] &= \\sum_{i=1}^n \\sum_{k=1}^K \\delta(z_i = k, x_i, \\Theta) \\log P(x_i, z_i = k | \\Theta) \\\\ &= \\sum_{i=1}^n \\sum_{k=1}^K \\delta(z_i = k, x_i, \\Theta) \\left(-\\frac{d}{2} \\log(2\\pi) + \\frac{1}{2}\\|x_i - \\theta_k\\|^2_2 \\right) \\end{align}$$\n",
    "\n",
    "Once we have this expression for the expected log-likelihood, we then optimize each of the cluster centroid by taking a mean over the points \"partially assigned\" to each cluster; essentially, this is a weighted average where the weights for each point $x_i$ are $\\delta(z_i = k, x_i, \\Theta)$.\n",
    "\n",
    "$$\\frac{\\partial \\mathbb{E}_{P(Z|X, \\Theta)}[\\log P(X, Z | \\Theta)]}{\\partial \\theta_k} = \\sum_{i=1}^n \\delta(z_i = k, x_i, \\Theta) (x_i - \\theta_k) = 0$$\n",
    "$$\\theta_k = \\frac{\\sum_{i=1}^n \\delta(z_i = k, x_i, \\Theta) x_i}{\\sum_{i=1}^n \\delta(z_i = k, x_i, \\Theta)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will wrap up this discussion with a toy EM clustering problem. Suppose that have 4 1-D data points that we are trying to cluster into two clusters. $X = \\{0, 1, 4, 5\\}$. We initialize $\\Theta = \\{2, 3\\}$.  Show the results of performing one round of expectation maximization, including the partial assignments of $Z$ and the recomputed parameters.\n",
    "\n",
    "$$P(Z_1 = 1 | X_1, \\Theta) = \\frac{\\exp(-\\frac{(0-2)^2}{2})}{\\exp(-\\frac{(0-2)^2}{2}) + \\exp(-\\frac{(0-3)^2}{2})} = \\frac{\\exp(-\\frac{4}{2})}{\\exp(-\\frac{4}{2}) + \\exp(-\\frac{9}{2})} = \\frac{\\exp(-2)}{\\exp(-2) + \\exp(-4.5)} = 0.9241$$\n",
    "\n",
    "$$P(Z_2 = 1 | X_2, \\Theta) = \\frac{\\exp(-\\frac{(1-2)^2}{2})}{\\exp(-\\frac{(1-2)^2}{2}) + \\exp(-\\frac{(1-3)^2}{2})} = \\frac{\\exp(-\\frac{1}{2})}{\\exp(-\\frac{1}{2}) + \\exp(-\\frac{4}{2})} = \\frac{\\exp(-0.5)}{\\exp(-0.5) + \\exp(-2)} = 0.8175$$\n",
    "\n",
    "$$P(Z_3 = 1 | X_3, \\Theta) = \\frac{\\exp(-\\frac{(4-2)^2}{2})}{\\exp(-\\frac{(4-2)^2}{2}) + \\exp(-\\frac{(4-3)^2}{2})} = \\frac{\\exp(-\\frac{4}{2})}{\\exp(-\\frac{4}{2}) + \\exp(-\\frac{1}{2})} = \\frac{\\exp(-2)}{\\exp(-2) + \\exp(-0.5)} = 0.1824$$\n",
    "\n",
    "$$P(Z_4 = 1 | X_4, \\Theta) = \\frac{\\exp(-\\frac{(5-2)^2}{2})}{\\exp(-\\frac{(5-2)^2}{2}) + \\exp(-\\frac{(5-3)^2}{2})} = \\frac{\\exp(-\\frac{9}{2})}{\\exp(-\\frac{9}{2}) + \\exp(-\\frac{4}{2})} = \\frac{\\exp(-4.5)}{\\exp(-4.5) + \\exp(-2)} = 0.0759$$\n",
    "\n",
    "The recomputed parameters are as follows.\n",
    "\n",
    "$$\\theta_1 = \\frac{0.9241(0) + 0.8175(1) + 0.1824(4) + 0.0759(5)}{0.9241 + 0.8175 + 0.1824 + 0.0759} = 0.9633$$\n",
    "\n",
    "$$\\theta_2 = \\frac{0.0759(0) + 0.1824(1) + 0.8175(4) + 0.9241(5)}{0.0759 + 0.1824 + 0.8175 + 0.9241} = 4.0366$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "The high-level goal of reinforcement learning is to learn a policy $\\Pi$ such that an agent taking actions based on this policy optimizes a reward function in a given environment. Here, we will review some of the basics of reinforcement learning, such as the definition of a Markov Decision Process, illustrate this with some examples, and show how this formulation naturally leads to the concepts of policy and value iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Markov Decision Process (MDP) is a discrete time step state transition system. It is used for sequential decision making under uncertainity and it takes into account short-term outcomes of current decisions and opportunities for making decisions in the future.\n",
    "\n",
    "It is modelled as follows:\n",
    "  1. **States**: A set of possible world states S\n",
    "  2. **Actions**: A set of possible actions A\n",
    "  3. **Reward**: A real valued reward function $r: S \\times A \\rightarrow R$\n",
    "  4. **Transition Function**: A description T of each action’s effects in each state $T: S \\times A \\times S \\rightarrow [0,1]$\n",
    "  Transition functions gives the probability of going from state $s$ to $s'$ under action $a$,\n",
    "  $T_a(s,s') = P(S_{t+1} = s' |s_t = s, a_t = a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Markov Assumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probability theory, Markov Property refers to memoryless property of a stochastic process. In other words the evidence at the current timestep only depends on the current state, not any previous states or previous evidence. \n",
    "\n",
    "$s_{t+1}$ and $r_{t+1}$ only depend on $s_t$ and $a_t$ but not on anything that happened before the timestep $t$.\n",
    "\n",
    "$$ P(s_{s+1}| s_t, a_t, s_{t-1}, a_{t-1}, \\ldots) =   P(s_{s+1}| s_t, a_t    )      $$\n",
    "$$ P(r_{s+1}| s_t, a_t, s_{t-1}, a_{t-1}, \\ldots) =   P(r_{s+1}| s_t, a_t    )      $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the MDP model, the goal is to learn a policy that maximizes the expected discounted return.\n",
    "\n",
    "$$ \\mathbf{E}\\left\\{r_{t}+\\gamma r_{t+1}+\\gamma^{2} r_{t+2}+\\cdots | s_{t}=s ; \\pi\\right\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\gamma \\in [0,1]$ for every possible state $s_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us look at some examples and answer the following questions:\n",
    "1. What are the states? \n",
    "2. What are the actions?\n",
    "3. How do we define the reward?\n",
    "4. What is the return?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Moutain Car"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/mountain_car.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. States: Position And Velocity\n",
    "2. Actions: Accelerate Forward, Accelerate Backward, Coast\n",
    "3. Reward Formulations: -1 for every timestep until the car reaches the goal at the top\n",
    "4. Reward Formulations: 1 at the top, 0 otherwise, $\\gamma = [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Cart Pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/cart_pole.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. States:\n",
    "    * Angle of the pole relative to the cart\n",
    "    * Angular velocity of the pole relative to the cart\n",
    "    * Position of the cart along the track\n",
    "    * Velocity of the cart along the track\n",
    "2. Actions: Accelerate Forward, Accelerate Backward, Coast\n",
    "3. Reward Formulations: 1 for every timestep until the cart falls\n",
    "4. Reward Formulations: -1 for failure, 0 otherwise, $\\gamma = [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a model, we use dynamic programming to determine the optimal policy. We follow a policy by determining the current state $s_t$ and executing the action $\\pi(s)$ to go to the next state $s_{t+1}$ and we repeat this process. We assume full observability i.e. the new state resulting from executing an action will be known to the system. For evaluating a policy for deterministic actions, we simply sum all the rewards. In the case of stochastic actions, we use the expected total reward. These two strategies can lead to infinite values and we mitigate that using the following:\n",
    "1. We may set a finite horizon and then take the total of all the rewards obtained using a particular policy. \n",
    "2. We can use discounting to leveraging the rewards obtained in the earlier steps. \n",
    "3. We take the average reward rate, given a limit. \n",
    "\n",
    "Most commonly used strategy is discounting where the reward for the $n^{th}$ step is discounted by a factor of $\\gamma^n \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Function : Finding a Policy Given a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value function $V_{\\pi} : S \\rightarrow R$ represents the expected objective value obtained following policy from each state $s_t$ in $S$.\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "V_{\\pi}(s) &=\\mathbf{E}\\left\\{r_{t}+\\gamma r_{t+1}+\\gamma^{2} r_{t+2}+\\cdots | s_{t}=s ; \\pi\\right\\} \\\\\n",
    "&=\\mathrm{E}\\left\\{\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t} | s_{t}=s ; \\pi\\right\\}\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a value of the discounting factor $\\gamma$ to be less than 1 to ensure convergence of the sum. For every start state $s$ we may investigate what the best policy $\\pi$ is and what its value would be. Let us define the otimal value function as:\n",
    "\n",
    "$$ V^{*}(s) = max_{\\pi} V^{\\pi}(s)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every MDP, there exists an optimal policy. There may exist multiple optimal policies, which each gain the optimal value for every start state. Further, there also exists at least one deterministic optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: Grid World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figures/grid_world.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define the state of the agent as $(x,y,o)$ where $x$ and $y$ are thehorizontal and vertical coordinates of the agent’s location, respectively, and $o$ is the agent’s orientation which is one of {N,E,W,S}(North, East, West, South). The agent’s current orientation is the only direction it can move. The agent is able to move forward, turn right, or turn left (deterministically).  All actions are available in all states. Walls are represented by thick black lines. The agent cannot move through walls. If the agent attempts tomove through a wall, it will remain in the same state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent gets a higher reward for the location with more dollarinos. Now answer the following questions:\n",
    "    1. Describe a suitable reward function. \n",
    "    2. Describe a discount factorγ that will lead to an optimal policy that follows the shortest path (i.e.,path using fewest actions) to the dollarinos from any initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
