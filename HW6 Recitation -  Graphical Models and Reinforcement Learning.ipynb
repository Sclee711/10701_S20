{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphical Models\n",
    "The central question that graphical models try to answer is how to more compactly write the joint distribution $P(X_1, X_2, ..., X_n)$ by accounting for the conditional independence relationships between variables. This is referred to as \"factoring\" the joint distribution. To emphasize the utility of doing so, consider that representing the full joint distribution (necessary when each variables always depends on all other variables) requires representing exponentially many states. In practice, variables may have some independence structure or relatively limited interaction with one another. Taking advantage of these conditions enables us to represent the joint distribution more or less equivalently with significantly fewer parameters. We will review a few common examples of how this is done, and then go over a problem involving a specific Bayesian Network. While there are many types of graphical models, we will only focus on directed graphical models whose structure is a directed acyclic graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "Suppose that we are considering how to classify vectors $X \\in \\{0, 1\\}^d$ into binary labels $Y \\in \\{0, 1\\}$. In general, we are trying to model the distribution $P(Y|X_1,X_2,...,X_d)$ which can be related to the joint distribution via $P(X_1,X_2,...,X_d,Y) = P(Y|X_1,X_2,...,X_d)P(X_1,X_2,...,X_d)$. However, Naive Bayes relies on a particular conditional independence assumption, namely that the features are conditionally independent given the label. This assumption allows us to write the conditional distribution of $X|Y$ as $P(X_1,X_2,...,X_d|Y) = \\prod_{i=1}^d P(X_i|Y)$. The graphical model corresponding to Naive Bayes is as follows.\n",
    "\n",
    "<img src=\"figures/naive_bayes.png\">\n",
    "\n",
    "While the Naive Bayes assumption is unlikely to exactly hold, it gives us a model that is tractable to represent (requiring only $2n + 1$ variables in the given binary case) and fairly robust in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden Markov Models (HMMs)\n",
    "Here, we'll discuss another type of graphical model commonly used for modeling sequences of data. Suppose instead that we have a sequence $X \\in \\{0, 1\\}^n$ (unobserved \"hidden\" variables that we are trying to model) which produces an observable binary output $Y_i$ at each step $i$ conditioned on all previous information. We thus have a sequence of prediction problems based on the following joint distribution, which we will later show how to simplify.\n",
    "\n",
    "$$P(X_1,X_2,...,X_n,Y_1,Y_2,...,Y_n) = \\prod_{i=1}^n P(X_i, Y_i|X_{i-1}, Y_{i-1}, ...,X_1, Y_1)$$\n",
    "\n",
    "As we can see, the states of this model depend on all past data, which becomes larger with every step. A common simplification (such as that adopted by n-gram models for natural language) is to reduce that dependence to a fixed window of size K. We will show how HMMs significantly reduce the problem space by only using a window of size 1 and adding a Markov assumption on the observed outputs.\n",
    "\n",
    "First, we can apply the Markov assumption - that $X_i$ is dependent only on $X_{i=1}$ - to simplify the probability distribution of $P(X_i|X_{i-1}, Y_{i-1}...,X_1, Y_1) = P(X_i|X_{i-1})$. Next, we extend this assumption to $Y$ by making $Y_i$ conditionally independent of all previous information given $X_i$, and hence $P(Y_i|X_i,X_{i-1}, Y_{i-1}, ...,X_1, Y_1) = P(Y_i|X_i)$. The graphical model now takes on the following form.\n",
    "\n",
    "<img src=\"figures/hmm.png\">\n",
    "\n",
    "The joint distribution thus simplifies to $P(X_1,X_2,...,X_n,Y_1,Y_2,...,Y_n) = \\prod_{i=1}^n P(Y_i|X_i)P(X_i|X_{i-1})$. Inference can then be performed in linear time using a dynamic programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Networks and Conditional Independence Rules\n",
    "Consider the some conditional independence tests involving the two following Bayes Nets.\n",
    "<img src=\"figures/ci_example1.png\">\n",
    "#### Are A and B independent? (A $\\perp$ B?) \n",
    "Yes.\n",
    "\n",
    "#### Are A and B conditionally independent given C? (A $\\perp$ B $|$ C?)\n",
    "No. Because both A and B influence C, knowing B and C affects the distribution of A.\n",
    "\n",
    "<img src=\"figures/ci_example2.png\">\n",
    "\n",
    "#### Are E and F independent? (E $\\perp$ F?) \n",
    "No. Knowing F affects the possible values of D, which affects $P(E|D)$.\n",
    "\n",
    "#### Are E and F conditionally independent given D? (E $\\perp$ F $|$ D?)\n",
    "Yes. The values of E and F only depend on $P(E|D)$ and $P(F|D)$.\n",
    "\n",
    "\n",
    "There are two main rules that this example illustrates. The first involves the relationship between nodes and parents. The value of a node can be affected by conditioning on any descendent of its parents, because observing a parent's descendent can provide information about the parent. However, if the parent's value is already known, then the parent's non-shared descendents will have no effect on the given node. Finally, conditioning on all of a node's parents will make it conditionally independent of all of its non-descendents.  \n",
    "\n",
    "The second rule involves the relationship between nodes and their children. If the only observed node is one of the node's descendents then it becomes dependent on all ancestors of the observed node. This is because all ancestors of a node affect the value of the observed node. Suppose we restrict the discussion to only the node's immediate children. Observing any of its children will make the node dependent on their children's other ancestors. However, the effect can be limited the node's children are also observed. \n",
    "\n",
    "Putting these rules together helps us derive the concept of a Markov Blanket. Given the values of a node's parents, the values of its children and its children's parents, the node will be conditionally independent of all other nodes in the graph, which also forms the basis of Gibbs Sampling.\n",
    "\n",
    "<img src=\"figures/bayes_net.png\">\n",
    "\n",
    "#### What is the Markov Blanket of Alarm?\n",
    "This includes all nodes except for Report, which is not needed because Alarm is conditionally independent of Report given Earthquake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: K-ZigZag Network\n",
    "\n",
    "A $k$-$zigzag$ network has $k$ Boolean root variables and $k + 1$ Boolean leaf variables, where root $i$ is connected to leaves $i$ and $i + 1$. Here is an example for $k = 3$, where each $D_i$ represents a Boolean disease variable and each $S_j$ is a Boolean symptom variable:\n",
    "\n",
    "<img src=\"figures/k_zigzag.png\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does having symptom $S_4$ affect the probability of disease $D_1$? Why or why not?\n",
    "No, because $S_4$ is not a descendant of $D_1$, and $D_1$ is independent of its nondescendants given its parents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using only conditional probabilities from the model, express the probability of having symptom $S_1$ but not symptom $S_2$, given disease $D_1$.\n",
    "$P(S_1, \\neg S_2|D_1) = P(S_1|D_1)P(\\neg S_2|D_1) = P(S_1|D_1)\\sum_{d2} P(d_2)P(S_2| D_1, d_2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $True/False$: Exact inference in a k-zigzag net can be done in time O(k).\n",
    "True, one can simply traverse the $2k - 1$ nodes one at a time and sum over the two possible states. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose the values of all the symptom variables have been observed, and you would like to do Gibbs sampling on the disease variables (i.e., sample each variable given its Markov blanket). What is the largest number of nonevidence variables that have to be considered when sampling any particular disease variable? Explain your answer\n",
    "\n",
    "2 non-evidence variables - the Markov blanket consists of its children (2 evidence variables) and their other parents (2 non-evidence variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization (in the context of graphical models)\n",
    "So far, we've mainly been concerned with the question of how to perform inference when the graph and the underlying probabilities are already known. We now turn our attention to the task of learning the probability tables for a known graph structure from data. In particular, we consider the case where the data that we're learning from contains missing entries. It is fairly straightforward to learn these tables when the data is not missing entries; one simply populates the tables with the empirical averages via the principle of maximum of likelihood. How do we properly compute these averages with missing data? At a high level, the solution adopted by EM is to assign preliminary (partial) values to the missing entries, evaluate the resulting likelihood, and then iteratively re-assign these partial values to optimize the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
